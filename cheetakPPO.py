# ğŸ§  PPO í•™ìŠµ, ì €ì¥, ë¡œë”©, ì‹œë®¬ë ˆì´ì…˜ ì‹œê°í™”ê¹Œì§€ ì „ì²´ ê³¼ì •

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
import gymnasium as gym
import time
import os

# ------------------------------------
# 1ï¸âƒ£ í™˜ê²½ ìƒì„± (ë Œë”ë§ì€ ì—†ì´ í•™ìŠµë§Œ)
env = gym.make("HalfCheetah-v4")  # í•™ìŠµ ì‹œì—ëŠ” render_mode ì—†ìŒ
vec_env = DummyVecEnv([lambda: env])  # SB3ëŠ” VecEnv í˜•íƒœ í•„ìš”
vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)  # ê´€ì¸¡ê°’ ì •ê·œí™”

# ------------------------------------
# 2ï¸âƒ£ PPO ì—ì´ì „íŠ¸ ì •ì˜ ë° í•™ìŠµ
model = PPO(
    "MlpPolicy",
    vec_env,
    verbose=1,
    ent_coef=0.01,          # íƒí—˜ì„ ë” í•˜ë„ë¡ ìœ ë„
    tensorboard_log="./ppo_cheetah_tensorboard/"
)

model.learn(total_timesteps=500_000)  # ğŸ’¡ ì¶©ë¶„íˆ ê¸´ í•™ìŠµ ì‹œê°„ í™•ë³´

# ------------------------------------
# 3ï¸âƒ£ ëª¨ë¸ ë° í™˜ê²½ ì €ì¥
model.save("ppo_halfcheetah")
vec_env.save("ppo_halfcheetah_env.pkl")  # VecNormalize íŒŒë¼ë¯¸í„° ì €ì¥

# ------------------------------------
# 4ï¸âƒ£ ì‹œë®¬ë ˆì´ì…˜ ë³´ê¸°ìš© í™˜ê²½ ìƒì„± (ë Œë”ë§ í¬í•¨)
eval_env = gym.make("HalfCheetah-v4", render_mode="human")
eval_vec_env = DummyVecEnv([lambda: eval_env])
eval_vec_env = VecNormalize.load("ppo_halfcheetah_env.pkl", eval_vec_env)
eval_vec_env.training = False  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
eval_vec_env.norm_reward = False

# ------------------------------------
# 5ï¸âƒ£ ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° í™˜ê²½ ì„¸íŒ…
loaded_model = PPO.load("ppo_halfcheetah")
loaded_model.set_env(eval_vec_env)

# ------------------------------------
# â–¶ï¸ ì‹œë®¬ë ˆì´ì…˜ ë° ë¦¬ì›Œë“œ ì¸¡ì •
obs = eval_vec_env.reset()
episode_reward = 0
episode_count = 0

for step in range(1000):
    action, _ = loaded_model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = eval_vec_env.step(action)

    # VecNormalize í™˜ê²½ì´ê¸° ë•Œë¬¸ì— rewardëŠ” arrayë¡œ ë‚˜ì˜´
    episode_reward += reward[0]

    time.sleep(0.01)

    if terminated or truncated:
        print(f"âœ… Episode {episode_count + 1} reward: {episode_reward:.2f}")
        episode_reward = 0
        episode_count += 1
        obs = eval_vec_env.reset()
